# Database Configuration
DATABASE_URL=postgresql://article_mind:article_mind@localhost:5432/article_mind

# API Configuration
API_V1_PREFIX=/api/v1
CORS_ORIGINS=http://localhost:5173

# CORS Configuration
# WARNING: DEVELOPMENT ONLY - Setting CORS_ALLOW_ALL=true allows ALL origins (*)
# This disables CORS protection and should NEVER be used in production
# Only enable for local development when testing from different network addresses
CORS_ALLOW_ALL=false

# Development Settings
DEBUG=true
LOG_LEVEL=INFO

# Embedding Provider Configuration
# Options: openai | ollama
EMBEDDING_PROVIDER=openai

# OpenAI Configuration (required if EMBEDDING_PROVIDER=openai)
# Get your API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-your-api-key-here

# Ollama Configuration (required if EMBEDDING_PROVIDER=ollama)
# Ensure Ollama is running locally: ollama serve
# Pull model first: ollama pull nomic-embed-text
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=nomic-embed-text

# ChromaDB Configuration
# Path for persistent vector storage
CHROMADB_PATH=./data/chromadb

# Chunking Configuration
# Chunk size in tokens (512 recommended for RAG)
CHUNK_SIZE=512
# Overlap between chunks in tokens (10% of chunk_size)
CHUNK_OVERLAP=50

# ===== LLM Configuration =====
# Provider: "openai" or "anthropic"
LLM_PROVIDER=openai

# API Keys (only one needed based on provider)
# OpenAI API Key (get from: https://platform.openai.com/api-keys)
# OPENAI_API_KEY is already configured above for embeddings

# Anthropic API Key (get from: https://console.anthropic.com/)
ANTHROPIC_API_KEY=sk-ant-your-api-key-here

# Model selection (provider-specific)
# OpenAI: gpt-4o-mini, gpt-4o, gpt-4-turbo
# Anthropic: claude-sonnet-4-5-20241022, claude-opus-4-5-20251101
LLM_MODEL=gpt-4o-mini

# Generation settings
LLM_MAX_TOKENS=2048

# ===== RAG Configuration =====
# Number of context chunks to include in prompt
RAG_CONTEXT_CHUNKS=5
